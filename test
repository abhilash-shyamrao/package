import sys
import boto3
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from awsglue.utils import getResolvedOptions

# Retrieve arguments (S3 bucket name)
args = getResolvedOptions(sys.argv, ['S3_BUCKET_NAME', 'S3_OUTPUT_PATH'])

s3_bucket_name = args['S3_BUCKET_NAME']
s3_output_path = args['S3_OUTPUT_PATH']

# Generate 40 unique column names
columns = [f'col_{i}' for i in range(1, 41)]

# Generate 10 rows of dummy data
data = {col: [f'data_{i}_{col}' for i in range(10)] for col in columns}

# Create a Pandas DataFrame
df = pd.DataFrame(data)

# Convert to Arrow Table
table = pa.Table.from_pandas(df)

# Define Parquet file path
parquet_file = "/tmp/dummy_data.parquet"

# Write DataFrame to Parquet file
pq.write_table(table, parquet_file)

# Upload to S3
s3_client = boto3.client('s3')
s3_client.upload_file(parquet_file, s3_bucket_name, f"{s3_output_path}/dummy_data.parquet")

print(f"Parquet file uploaded to s3://{s3_bucket_name}/{s3_output_path}/dummy_data.parquet")
